<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.main API documentation</title>
<meta name="description" content="Main file to train and evaluate models with distributed optimizers such as Signum." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.main</code></h1>
</header>
<section id="section-intro">
<p>Main file to train and evaluate models with distributed optimizers such as Signum.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Main file to train and evaluate models with distributed optimizers such as Signum.&#34;&#34;&#34;


# Import Python packages
import argparse
import numpy as np
import torch
import torch.multiprocessing as mp
from typing import Callable

# Import our own packages
import utils as ut
from model_training import train_process


def run(fn: Callable, world_size: int, blind_inv_adv: np.ndarray, byz_adv: np.ndarray, seed: int,
        data_type: str, model_name: str, optim_name: str, n_epochs: int = 10,
        save_score: bool = True, verbose: bool = True):
    &#34;&#34;&#34;
    Create processes and run models.

    Parameters
    ----------
    fn : Callable
        A function containing all steps to proceed with.

    world_size : int
        Total amount of processes

    blind_inv_adv : np.ndarray
        Array containing the ranks of blind adversaries that invert their gradients signs.

    byz_adv : np.ndarray
        Ranks of Byzantine adversaries.

    seed : int
        Global seed.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
        Name of the optimizer.

    n_epochs : int, default=10
        Number of training epochs.

    save_score : bool, default=True
        If True, scores (loss and accuracy) for each epoch will be saved in csv files.

    verbose : boool, default=True
        If True, will print all metrics of all processes along epochs. If False, will only show a
        progress bar for the rank 0 process.
    &#34;&#34;&#34;
    fn_args = (world_size, blind_inv_adv, byz_adv, seed, data_type, model_name, optim_name,
               n_epochs, save_score, verbose)

    mp.spawn(train_process, args=fn_args, nprocs=world_size, join=True, start_method=&#34;spawn&#34;)


if __name__ == &#34;__main__&#34;:

    if not torch.distributed.is_available():
        error_msg = &#34;PyTorch support for distributed applications is not enabled on your machine.&#34;
        raise ValueError(error_msg)

    # Command lines
    parser_desc = &#34;Main file to train and evaluate the SignSGD optimizer.&#34;
    parser = argparse.ArgumentParser(description=parser_desc)

    # Number of processes
    parser.add_argument(&#34;nprocs&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Choose the total amount of processes to run in parallel. This number \
                             contains healthy workers aswell as possible adversaries to be set up \
                             with optional arguments.
                             &#34;&#34;&#34;)

    parser.add_argument(&#34;-i&#34;,
                        &#34;--blindinv&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Choose a NUMBER of blind adversaries inverting their gradients \
                             signs. Default: 0.
                             &#34;&#34;&#34;,
                        default=0)

    parser.add_argument(&#34;-b&#34;,
                        &#34;--byzantine&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Choose a NUMBER of Byzantine adversaries. This number must not be \
                             greater than nprocs minus the number of blind adversaries. Default: 0.
                             &#34;&#34;&#34;,
                        default=0)

    # Seed selection
    parser.add_argument(&#34;-s&#34;,
                        &#34;--seed&#34;,
                        type=int,
                        help=&#34;&#34;&#34;Choose a global seed shared by all processes. Default: 42.&#34;&#34;&#34;,
                        default=42)

    # Data type and corresponding neural network &amp; optimizer
    parser.add_argument(&#34;dataset&#34;,
                        type=str,
                        help=&#34;&#34;&#34;Choose the name of the dataset.&#34;&#34;&#34;,
                        choices=[&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;])

    parser.add_argument(&#34;-n&#34;,
                        &#34;--net&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             Choose the name of the neural network to use for training. \
                             &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible with MNIST dataset, while \
                             &#34;ResNet18&#34; and &#34;ResNet50&#34; are compatible with ImageNet. &#34;LinRegNet&#34; \
                             is only compatible with LinReg dataset, as well as &#34;LogRegNet&#34; and \
                             LogReg. Default: &#34;MNISTNet&#34;.
                             &#34;&#34;&#34;,
                        choices=[&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;,
                                 &#34;LogRegNet&#34;],
                        default=&#34;MNISTNet&#34;)

    parser.add_argument(&#34;-o&#34;,
                        &#34;--optimizer&#34;,
                        type=str,
                        help=&#34;&#34;&#34;Choose an optimizer. Default: &#34;Signum&#34;.&#34;&#34;&#34;,
                        choices=[&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;],
                        default=&#34;Signum&#34;)

    # Number of training epochs
    parser.add_argument(&#34;-e&#34;,
                        &#34;--epochs&#34;,
                        type=int,
                        help=&#34;&#34;&#34;Number of training epochs. Default: 10.&#34;&#34;&#34;,
                        default=10)

    # Saving results
    parser.add_argument(&#34;-m&#34;,
                        &#34;--metrics&#34;,
                        type=str,
                        help=&#34;&#34;&#34;Choose whether to save scores after each epoch. Default: True.&#34;&#34;&#34;,
                        default=&#34;True&#34;)

    parser.add_argument(&#34;-v&#34;,
                        &#34;--verbose&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             If True, will print all metrics of all processes along epochs. \
                             If False, will only show a progress bar for the rank 0 process.
                             Default: False.
                             &#34;&#34;&#34;,
                        default=&#34;False&#34;)

    # End of command lines
    args = parser.parse_args()

    # Set global seed
    seed = args.seed
    ut.set_seed(seed)

    # Data and neural network
    data_type = args.dataset
    model_name = args.net
    optim_name = args.optimizer

    data_type, model_name, optim_name = ut.check_params(data_type, model_name, optim_name)

    # Number of processes
    world_size = args.nprocs
    print(&#34;In total, there are {} processes.&#34;.format(world_size))

    # Establish which processes are blind adversaries that invert their gradients signs
    blind_inv_size = args.blindinv
    print(&#34;{} blind adversaries are inverting their gradients signs.&#34;.format(blind_inv_size))
    blind_inv_adv = np.random.choice(world_size, size=blind_inv_size)

    # Establish which processes are Byzantine adversaries
    byz_adv_size = args.byzantine
    print(&#34;{} Byzantine adversaries can send arbitrary values.&#34;.format(byz_adv_size))
    byz_adv = np.random.choice([rank for rank in range(world_size) if rank not in blind_inv_adv],
                               size=byz_adv_size)

    # Training parameters
    n_epochs = args.epochs
    save_score = ut.str2bool(args.metrics)
    verbose = ut.str2bool(args.verbose)

    run(train_process, world_size, blind_inv_adv, byz_adv, seed, data_type, model_name, optim_name,
        n_epochs=n_epochs, save_score=save_score, verbose=verbose)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.main.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>fn: Callable, world_size: int, blind_inv_adv: numpy.ndarray, byz_adv: numpy.ndarray, seed: int, data_type: str, model_name: str, optim_name: str, n_epochs: int = 10, save_score: bool = True, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Create processes and run models.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fn</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function containing all steps to proceed with.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes</dd>
<dt><strong><code>blind_inv_adv</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array containing the ranks of blind adversaries that invert their gradients signs.</dd>
<dt><strong><code>byz_adv</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Ranks of Byzantine adversaries.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Global seed.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"MNIST", "ImageNet", "LinReg", "LogReg"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"TorchNet", "MNISTNet", "ResNet18", "ResNet50", "LinRegNet", "LogRegNet"}</code></dt>
<dd>Name of the neural network to use for training. "TorchNet" and "MNISTNet" are compatible
with "MNIST" dataset, while "ResNetX" are compatible with "ImageNet". "LinRegNet" is only
compatible with "LinReg" dataset, as well as "LogRegNet" and "LogReg".</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"DistSGD", "Signum", "SignSGD"}</code></dt>
<dd>Name of the optimizer.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>Number of training epochs.</dd>
<dt><strong><code>save_score</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, scores (loss and accuracy) for each epoch will be saved in csv files.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boool</code>, default=<code>True</code></dt>
<dd>If True, will print all metrics of all processes along epochs. If False, will only show a
progress bar for the rank 0 process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(fn: Callable, world_size: int, blind_inv_adv: np.ndarray, byz_adv: np.ndarray, seed: int,
        data_type: str, model_name: str, optim_name: str, n_epochs: int = 10,
        save_score: bool = True, verbose: bool = True):
    &#34;&#34;&#34;
    Create processes and run models.

    Parameters
    ----------
    fn : Callable
        A function containing all steps to proceed with.

    world_size : int
        Total amount of processes

    blind_inv_adv : np.ndarray
        Array containing the ranks of blind adversaries that invert their gradients signs.

    byz_adv : np.ndarray
        Ranks of Byzantine adversaries.

    seed : int
        Global seed.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
        Name of the optimizer.

    n_epochs : int, default=10
        Number of training epochs.

    save_score : bool, default=True
        If True, scores (loss and accuracy) for each epoch will be saved in csv files.

    verbose : boool, default=True
        If True, will print all metrics of all processes along epochs. If False, will only show a
        progress bar for the rank 0 process.
    &#34;&#34;&#34;
    fn_args = (world_size, blind_inv_adv, byz_adv, seed, data_type, model_name, optim_name,
               n_epochs, save_score, verbose)

    mp.spawn(train_process, args=fn_args, nprocs=world_size, join=True, start_method=&#34;spawn&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.main.run" href="#src.main.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>